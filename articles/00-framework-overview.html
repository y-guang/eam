<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Overview of the EAM Framework ‚Ä¢ eam</title>
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><script src="../extra.js"></script><meta property="og:title" content="Overview of the EAM Framework">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">eam</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tutorials</h6></li>
    <li><a class="dropdown-item" href="../articles/00-framework-overview.html">Overview of the EAM Framework</a></li>
    <li><a class="dropdown-item" href="../articles/10-minimal-working-example.html">Getting Started: A Minimal Working Example</a></li>
    <li><a class="dropdown-item" href="../articles/20-models-in-eam.html">Models in eam</a></li>
    <li><a class="dropdown-item" href="../articles/30-empirical-example.html">An Empirical Example: Simulation-Based Inference for Free Recall</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Developer Guide</h6></li>
    <li><a class="dropdown-item" href="../articles/developer-guide.html">Developer Guide</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Chinese Translation</h6></li>
    <li><a class="dropdown-item" href="../articles/about-zh-translation.html">About Chinese Translation</a></li>
    <li><a class="dropdown-item" href="../articles/developer-guide.zh.html">ÂºÄÂèë‰∫∫ÂëòÂºïÂØº</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/y-guang/eam/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Overview of the EAM Framework</h1>
                        <h4 data-toc-skip class="author">Guangyu
Zhu</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/y-guang/eam/blob/main/vignettes/00-framework-overview.Rmd" class="external-link"><code>vignettes/00-framework-overview.Rmd</code></a></small>
      <div class="d-none name"><code>00-framework-overview.Rmd</code></div>
    </div>

    
    
<p>Evidence accumulation models (EAMs) are major approaches for modeling
responses and reaction times in cognitive tasks. The
<strong>eam</strong> package provide a simulation-based framework for
simulating and fitting EAMs for single- and multiple-response tasks.</p>
<p>The <strong>eam</strong> package consists of two parts:</p>
<p> that allows users to flexibly customize models and simulate
data;</p>
<p> for reliable parameter inference without requiring a tractable
likelihood.</p>
<p>In this section, we introduce these two components in detail and
demonstrate how they can be combined to build, simulate, and fit
customized evidence accumulation models.</p>
<div class="section level2">
<h2 id="modular-simulation-engine">Modular simulation engine<a class="anchor" aria-label="anchor" href="#modular-simulation-engine"></a>
</h2>
<p>The modular simulation engine is built upon a generalized evidence
accumulation framework, within which we assume that decision making is
characterized as a stochastic process in which evidence is continuously
collected over time. The accumulation unfolds in infinitesimal
increments
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">dt</annotation></semantics></math>,
which can be approximated in practice with very small time steps (for
example, 1,). The moment-to-moment change in accumulated evidence
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>
can be expressed as a stochastic differential:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>v</mi><mo>‚ãÖ</mo><mi>d</mi><mi>t</mi><mo>+</mo><mi>s</mi><mo>‚ãÖ</mo><mi>œÉ</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
dx(t) = v \cdot dt + s \cdot \sigma,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
is the drift rate,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">dt</annotation></semantics></math>
is a time step,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
is the noise scaling parameter, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
is a diffusion noise term.</p>
<p>As the process unfolds, the accumulated evidence
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>
continues to evolve until it reaches the decision boundaries, at which
point a response is triggered and the corresponding RT is recorded. The
distance to reaching the boundary is jointly determined by two
parameters: the decision boundary
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>)
and the starting point
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>).
As evidence accumulates, the process continues until
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">x(t) + z</annotation></semantics></math>
first reaches either the upper boundary
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
or falls below the lower boundary 0 (in two-boundary EAMs). The elapsed
time from the start of accumulation to this boundary crossing defines
the decision time; adding a non-decision time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mn>0</mn></msub><annotation encoding="application/x-tex">t_{0}</annotation></semantics></math>
yields the observable RT.</p>
<div class="float">
<img src="00-framework-overview/procedures.svg" alt="Procedures of simulation engines in the eam package"><div class="figcaption">Procedures of simulation engines in the eam
package</div>
</div>
<p>The modular simulation engine composes ten modules (see Figure
1):</p>
<p>: the number of accumulators that race or compete toward a decision
boundary (e.g., one in DDM or LFM; multiple in LBA, LCA, or RDM)</p>
<p>: hierarchical priors specifying mean, variance, and regression
coefficients for core parameters of the evidence-accumulation process
including drift rate, decision boundary, starting point/relative bias,
non-decision time, leakage parameter, strength of lateral inhibition,
and stability parameter.</p>
<p>: specify the formulas for between-subject or between-trial
variability in parameters, also allowing linking covariates to the
subject- or trial-level parameters.</p>
<p>: linking each accumulator‚Äôs parameter to item indexes or output
positions to generate the accumulator-level parameter values such as
drift rate or decision boundary, also allowing linking covariates to the
item-level parameters.</p>
<p>: family of diffusion noise (e.g., Gaussian vs.¬†L√©vy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>-stable
vs.¬†deterministic ballistic).</p>
<p>: the configuration used for model simulation, including model
equations, number of conditions, number of trials per condition, and
other implementation parameters.</p>
<p>: the criteria determining when and how responses are recorded (e.g.,
first boundary crossing only, or the first
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
releases within a time limit).</p>
<p>: the discrete increment used to approximate continuous-time evidence
accumulation (default at 1,).</p>
<p>: the algorithmic specification of stochastic increments (e.g.,
additive vs.¬†multiplicative).</p>
<p>: the underlying architecture of evidence accumulation (e.g.,
single-boundary vs.¬†double-boundary vs.¬†LCA accumulators).</p>
<p>By combining different modular components, the engine can simulate
responses and RTs from many representative EAMs (see the section <a href="./20-models-in-eam.html">Models in eam</a> for the configurations
of each representative EAM).</p>
</div>
<div class="section level2">
<h2 id="simulation-based-inference-module">Simulation-based inference module<a class="anchor" aria-label="anchor" href="#simulation-based-inference-module"></a>
</h2>
<p>The simulation-based inference module includes two major apporaches:
Approximate Bayesian Computation (ABC; Csill√©ry et al., 2012) and
Amortized Bayesian Inference (ABI; Sainsbury-Dale et al., 2024). These
methods can estimate the posteiror distribution of parameters via the
comparsion between simulated and observed data, bypassing the need for
likelihood functions.</p>
<div class="section level3">
<h3 id="approximate-bayesian-computation-abc">Approximate Bayesian Computation (ABC)<a class="anchor" aria-label="anchor" href="#approximate-bayesian-computation-abc"></a>
</h3>
<p>The Approximate Bayesian Computation (ABC) approximates the posterior
distribution by comparing summary statistics from simulated data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><annotation encoding="application/x-tex">y_{\text{sim}}</annotation></semantics></math>
and observed data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><annotation encoding="application/x-tex">y_{\text{obs}}</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>Œµ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àù</mo><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><msub><mi>K</mi><mi>Œµ</mi></msub><mspace width="-0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÅ</mi><mspace width="-0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="0.167em"></mspace><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
p_{\varepsilon}(\theta \mid y_{\text{obs}}) \propto 
\pi(\theta)\, K_{\varepsilon}\!\left(\rho\!\left(S(y_{\text{sim}}),\, 
S(y_{\text{obs}})\right)\right),
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(\cdot)</annotation></semantics></math>
denotes a vector of summary statistics,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÅ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\rho(\cdot)</annotation></semantics></math>
is a distance function, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>Œµ</mi></msub><annotation encoding="application/x-tex">K_{\varepsilon}</annotation></semantics></math>
is a kernel with tolerance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œµ</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>.</p>
<p>Here, the intractable likelihood is replaced by a kernel-weighted
similarity between summary statistics of simulated and observed data,
such that parameters producing simulations closer to the observed data
receive higher posterior weight, with the tolerance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œµ</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>
controlling the approximation accuracy.</p>
<p>In the classical Rejection ABC algorithm, for each iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">i=1,\dots,N</annotation></semantics></math>:
(1) draw
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>i</mi></msub><mo>‚àº</mo><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta_i \sim \pi(\theta)</annotation></semantics></math>;
(2) simulate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><mo>‚àº</mo><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y_{\text{sim}} \sim M(\theta_i)</annotation></semantics></math>;
(3) compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mi>œÅ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d_i = \rho(S(y_{\text{sim}}), S(y_{\text{obs}}))</annotation></semantics></math>;
(4) accept
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∏</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_i</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>‚â§</mo><mi>Œµ</mi></mrow><annotation encoding="application/x-tex">d_i \le \varepsilon</annotation></semantics></math>.</p>
<p>This produces the approximate posterior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÄ</mi><mi>Œµ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\varepsilon}(\theta \mid y)</annotation></semantics></math>.</p>
<p>However, the Rejection ABC algorithm suffers from low acceptance
rates and a bias‚Äìvariance trade-off in the tolerance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œµ</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>.
To address this, Beaumont et al.¬†(2002) proposed a local linear
regression adjustment:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>i</mi></msub><mo>=</mo><mi>Œ±</mi><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>Œ≤</mi><mo>+</mo><msub><mi>Œ∂</mi><mi>i</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
\theta_i = \alpha + 
\left(S(y_{\text{sim}}) - S(y_{\text{obs}})\right)^{T}\beta + \zeta_i,
</annotation></semantics></math> which is then adjusted toward
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(y_{\text{obs}})</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ∏</mi><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><msub><mi>Œ∏</mi><mi>i</mi></msub><mo>‚àí</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>Œ≤</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\theta_i^{\ast} = \theta_i -
\left(S(y_{\text{sim}}) - S(y_{\text{obs}})\right)^{T}\beta.
</annotation></semantics></math> This adjustment allows simulated
samples that would otherwise fall outside the tolerance to be shifted
into the acceptance region, which increases the acceptance rate and
reduces sensitivity to the choice of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>.</p>
<p>Following this direction, Ridge regression adjustment extends it by
projecting summary statistics into a high-dimensional RKHS using kernels
(e.g., Gaussian RBF), enabling nonlinear estimation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùîº</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>s</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[\theta \mid s]</annotation></semantics></math>.
Blum et al.¬†(2010) proposed a nonlinear, heteroscedastic regression
model:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∏</mi><mo>=</mo><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚ãÖ</mo><mi>Œ∂</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
\theta = m(s) + \sigma(s)\cdot \zeta,
</annotation></semantics></math> with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\cdot)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(\sigma^2(\cdot))</annotation></semantics></math>
estimated using neural networks with weight decay on accepted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ∏</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\theta_i, s_i)</annotation></semantics></math>
pairs. This correction improves robustness in high-dimensional summary
spaces and mitigates the curse of dimensionality.</p>
</div>
<div class="section level3">
<h3 id="amortized-bayesian-inference-abi">Amortized Bayesian Inference (ABI)<a class="anchor" aria-label="anchor" href="#amortized-bayesian-inference-abi"></a>
</h3>
<p>While methods such as ABC require running inference each time new ABI
allows one to first train a neural network that learns a mapping between
the parameter space and the data (or summary statistics), and then reuse
this learned mapping to directly infer parameters in subsequent analyses
without repeated optimization.</p>
<p>Existing implementations of amortized Bayesian inference (ABI) in
this package are neural Bayes estimators and neural posterior
inference.</p>
<p>Neural Bayes estimators aim to find decision rules that minimize the
Bayes risk under a chosen loss
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>,</mo><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta,\widehat{\theta}(Z))</annotation></semantics></math>
and prior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi(\theta)</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>min</mo><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover></munder><msub><mo>‚à´</mo><mi>Œò</mi></msub><msub><mo>‚à´</mo><mi>Z</mi></msub><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>,</mo><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Z</mi><mo>‚à£</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>Z</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>Œ∏</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
\min_{\widehat{\theta}}
\int_{\Theta}\int_{Z}
L(\theta,\widehat{\theta}(Z))\, p(Z \mid \theta)\,\pi(\theta)\, dZ\, d\theta,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is the latent true parameter and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Œ∏</mi><mo accent="true">ÃÇ</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\widehat{\theta}(Z)</annotation></semantics></math>
is the estimator produced from data (or summary statistics)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>.
The loss function may be chosen flexibly (e.g., quadratic, absolute,
quantile loss), leading to different Bayesian optimal decision rules
(e.g., posterior mean, median, quantiles). By optimizing the loss
function via backpropagation and stochastic gradient descent using
simulated
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>,</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\theta, Z)</annotation></semantics></math>
pairs, neural Bayes estimators amortize the cost of inference. Once
trained, the network can be applied repeatedly to new observed data to
produce fast and accurate point estimates with essentially zero
additional inference cost.</p>
<p>Neural posterior estimators, on the other hand, aim to approximate
the entire posterior distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\theta\mid Z)</annotation></semantics></math>
by learning a mapping from data to the parameters of a chosen
distribution family
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><mi>Z</mi><mo>‚Üí</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">\kappa(\cdot): Z \to K</annotation></semantics></math>
through minimization of the expected Kullback‚ÄìLeibler divergence between
the true posterior and the approximating distribution:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Œ∫</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mrow><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><msub><mi>ùîº</mi><mi>Z</mi></msub><mspace width="-0.167em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mi>K</mi><mi>L</mi><mspace width="-0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.278em"></mspace><mo stretchy="true" form="infix">‚à•</mo><mspace width="0.278em"></mspace><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>;</mo><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\kappa^{\ast}(\cdot)
  = \arg\min_{\kappa(\cdot)}
    \mathbb{E}_{Z}\!\left[
      KL\!\left( p(\theta\mid Z)\;\middle\|\; q(\theta;\kappa(Z)) \right)
    \right],
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Œ∫</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\kappa^{\ast}(\cdot)</annotation></semantics></math>
is the trained neural estimator,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>;</mo><mi>Œ∫</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\theta;\kappa(Z))</annotation></semantics></math>
is the approximated posterior family, and the expectation is taken over
all possible realizations of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="a-standard-workflow-in-the-eam-package">A standard workflow in the <strong>eam</strong> package<a class="anchor" aria-label="anchor" href="#a-standard-workflow-in-the-eam-package"></a>
</h2>
<p>A standard procedure of simulation-based inference involves several
steps:</p>
<p>First, a generative model or simulator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">M(\theta)</annotation></semantics></math>
is defined, which specifies how observable data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
are generated from underlying parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
Second, parameters are sampled from a prior distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi(\theta)</annotation></semantics></math>,
reflecting prior beliefs or theoretical constraints. Third, for each
sampled parameter value, synthetic datasets
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mtext mathvariant="normal">sim</mtext></msub><annotation encoding="application/x-tex">y_{\text{sim}}</annotation></semantics></math>
are simulated from the model, producing pairs of parameters and
corresponding simulated observations. Fourth, the similarity between
simulated and observed data is quantified‚Äîeither by computing distance
metrics between summary statistics, as in Approximate Bayesian
Computation (ABC) or Amortized Bayesian Inference (ABI). Finally, the
approximate posterior distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><msub><mi>y</mi><mtext mathvariant="normal">obs</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\theta \mid y_{\text{obs}})</annotation></semantics></math>
is obtained by weighting or sampling parameters according to this
similarity or likelihood estimate.</p>
<p><img src="00-framework-overview/workflow.svg" alt="The standard workflow in the eam package"></p>
<p>The following sections provide a structured overview of the standard
simulation and inference workflow supported by the package.</p>
<p> <a href="./10-minimal-working-example.html">Getting Started: A
Minimal Working Example</a> introduces the core functions of the package
and provides a minimal runnable example based on a three-parameter Drift
Diffusion Model (DDM).</p>
<p> <a href="./20-models-in-eam.html">Models in eam</a> demonstrates how
representative evidence accumulation models can be specified within the
package, and how they can be extended to multi-response settings or
intergeted with covariates.</p>
<p> <a href="./30-empirical-example.html">An Empirical Example:
Simulation-Based Inference for Free Recall</a> presents a real-world
application using free recall data, illustrating how the package can be
applied to empirical datasets in practice.</p>
<p>Reference:</p>
<p>Beaumont, M. A., Zhang, W., &amp; Balding, D. J. (2002). Approximate
Bayesian computation in population genetics. Genetics, 162 (4),
2025‚Äì2035. <a href="https://doi.org/10.1093/genetics/162.4.2025" class="external-link uri">https://doi.org/10.1093/genetics/162.4.2025</a></p>
<p>Blum, M. G. B., &amp; Fran√ßois, O. (2010). Non-linear regression
models for approximate Bayesian computation. Statistics and Computing,
20 (1), 63‚Äì73. <a href="https://doi.org/10.1007/s11222-009-9116-0" class="external-link uri">https://doi.org/10.1007/s11222-009-9116-0</a></p>
<p>Csill√©ry, K., Fran√ßois, O., &amp; Blum, M. G. B. (2012). abc: An R
package for approximate Bayesian computation (ABC). Methods in Ecology
and Evolution, 3 (3), 475‚Äì479. <a href="https://doi.org/10.1111/j.2041-210x.2011.00179.x" class="external-link uri">https://doi.org/10.1111/j.2041-210x.2011.00179.x</a></p>
<p>Sainsbury-Dale, M., Zammit-Mangion, A., &amp; Huser, R. (2024).
Likelihood-free parameter estimation with neural Bayes estimators. The
American Statistician, 78 (1), 1‚Äì14. <a href="https://doi.org/10.1080/00031305.2023.2275927" class="external-link uri">https://doi.org/10.1080/00031305.2023.2275927</a></p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Guangyu Zhu, Guang Yang.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
